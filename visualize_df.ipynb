{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a5e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb250af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_index.csv\")\n",
    "df_test = pd.read_csv(\"test_index.csv\")\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df = df[['filename', 'speaker', 'gender']]\n",
    "df['id'] = df.filename.apply(lambda x: x.split('-')[-5])\n",
    "df_filtered = df[df.groupby(\"speaker\")[\"id\"].transform(\"nunique\") == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_data(n_speakers=110, random_state=83):\n",
    "    \"\"\"\n",
    "    Choose speakers and prepare train-val split\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to preprocess.\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(\"train_index.csv\")\n",
    "    df_test = pd.read_csv(\"test_index.csv\")\n",
    "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    df = df[['filename', 'speaker', 'gender']]\n",
    "    # df['filename'] = df['filename'].str.replace('VOiCES_devkit', 'spectrograms_dataset', regex=False)\n",
    "    # df['filename'] = df['filename'].str.replace('.wav', '.pt', regex=False)\n",
    "    spk_df = df[['speaker', 'gender']].drop_duplicates()\n",
    "    # przydziel etykiety\n",
    "    allowed_idx = (\n",
    "        spk_df.groupby('gender', group_keys=False)\n",
    "                .apply(lambda x: x.sample(frac=n_speakers/spk_df.shape[0], random_state=random_state), include_groups=False)\n",
    "                ['speaker']\n",
    "    )\n",
    "    df  = df[df['speaker'].isin(allowed_idx)]\n",
    "    print(f\"Number of speakers in the dataset: {df['speaker'].nunique()}\")\n",
    "    print(f\"Number of segments in the dataset: {df.shape[0]}\")\n",
    "    spk_df = df[['speaker', 'gender']].drop_duplicates()\n",
    "    allowed_idx = (\n",
    "        spk_df.groupby('gender', group_keys=False)\n",
    "                .apply(lambda x: x.sample(frac=0.10, random_state=random_state), include_groups=False)\n",
    "                ['speaker']\n",
    "    )\n",
    "    allowed_df  = df[df['speaker'].isin(allowed_idx)]\n",
    "    non_allowed_df = df[~df['speaker'].isin(allowed_idx)]\n",
    "    allowed_df['label'] = 1\n",
    "    non_allowed_df['label'] = 0\n",
    "    # df = pd.concat([allowed_df, non_allowed_df], ignore_index=True)\n",
    "    # train-test split\n",
    "    allowed_df_train, allowed_df_test = train_test_split(\n",
    "        allowed_df,                      # Twój pełny DataFrame\n",
    "        test_size=0.2,           # 20 % w test / 80 % w train (zmień wedle potrzeb)\n",
    "        random_state=42,         # powtarzalność losowania\n",
    "        stratify=allowed_df['gender']  # << klucz stratyfikacji\n",
    "    )\n",
    "    allowed_df_val, allowed_df_test = train_test_split(\n",
    "        allowed_df_test,                      # Twój pełny DataFrame\n",
    "        test_size=0.5,           # 20 % w test / 80 % w train (zmień wedle potrzeb)\n",
    "        random_state=42,         # powtarzalność losowania\n",
    "        stratify=allowed_df_test['gender']  # << klucz stratyfikacji\n",
    "    )\n",
    "    non_allowed_df_train, non_allowed_df_test = train_test_split(\n",
    "        non_allowed_df,                      # Twój pełny DataFrame\n",
    "        test_size=0.2,           # 20 % w test / 80 % w train (zmień wedle potrzeb)\n",
    "        random_state=42,         # powtarzalność losowania\n",
    "        stratify=non_allowed_df['gender']  # << klucz stratyfikacji\n",
    "    )\n",
    "    non_allowed_df_val, non_allowed_df_test = train_test_split(\n",
    "        non_allowed_df_test,                      # Twój pełny DataFrame\n",
    "        test_size=0.5,           # 20 % w test / 80 % w train (zmień wedle potrzeb)\n",
    "        random_state=42,         # powtarzalność losowania\n",
    "        stratify=non_allowed_df_test['gender']  # << klucz stratyfikacji\n",
    "    )\n",
    "    train_df = pd.concat([allowed_df_train, non_allowed_df_train], ignore_index=True)\n",
    "    val_df = pd.concat([allowed_df_val, non_allowed_df_val], ignore_index=True)\n",
    "    test_df = pd.concat([allowed_df_test, non_allowed_df_test], ignore_index=True)\n",
    "    # sortowanie\n",
    "    train_df.to_csv(\"data/train_df1.csv\", index=False)\n",
    "    val_df.to_csv(\"data/val_df1.csv\", index=False)\n",
    "    test_df.to_csv(\"data/test_df1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cb3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_non_leakage(n_speakers=66, random_state=83):\n",
    "    \"\"\"\n",
    "    Choose speakers and prepare train-val split\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to preprocess.\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(\"train_index.csv\")\n",
    "    df_test = pd.read_csv(\"test_index.csv\")\n",
    "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    df = df[['filename', 'speaker', 'gender']]\n",
    "    df['id'] = df.filename.apply(lambda x: x.split('-')[-5])\n",
    "    df = df[df.groupby(\"speaker\")[\"id\"].transform(\"nunique\") == 2]\n",
    "\n",
    "    spk_df = df[['speaker', 'gender']].drop_duplicates()\n",
    "    # przydziel etykiety\n",
    "    allowed_idx = (\n",
    "        spk_df.groupby('gender', group_keys=False)\n",
    "                .apply(lambda x: x.sample(frac=n_speakers/spk_df.shape[0], random_state=random_state), include_groups=False)\n",
    "                ['speaker']\n",
    "    )\n",
    "    df  = df[df['speaker'].isin(allowed_idx)]\n",
    "    print(f\"Number of speakers in the dataset: {df['speaker'].nunique()}\")\n",
    "    print(f\"Number of segments in the dataset: {df.shape[0]}\")\n",
    "    spk_df = df[['speaker', 'gender']].drop_duplicates()\n",
    "    allowed_idx = (\n",
    "        spk_df.groupby('gender', group_keys=False)\n",
    "                .apply(lambda x: x.sample(frac=0.10, random_state=random_state), include_groups=False)\n",
    "                ['speaker']\n",
    "    )\n",
    "    allowed_df  = df[df['speaker'].isin(allowed_idx)]\n",
    "    non_allowed_df = df[~df['speaker'].isin(allowed_idx)]\n",
    "    allowed_df['label'] = 1\n",
    "    non_allowed_df['label'] = 0\n",
    "    # df = pd.concat([allowed_df, non_allowed_df], ignore_index=True)\n",
    "    # train-test split po dwóch rodzajach nagrania dla każdego speakera\n",
    "    # allowed speakers\n",
    "    allowed_df[\"grp\"] = (\n",
    "        allowed_df.groupby(\"speaker\")[\"id\"]          # grupujemy po speaker\n",
    "        .transform(lambda x: pd.factorize(x)[0])  # 0 dla 1. id, 1 dla 2. id\n",
    "    )\n",
    "    allowed_df_train = allowed_df[allowed_df[\"grp\"] == 0].drop(columns=\"grp\").copy()\n",
    "    allowed_df_test = allowed_df[allowed_df[\"grp\"] == 1].drop(columns=\"grp\").copy()\n",
    "    # non-allowed speakers\n",
    "    non_allowed_df[\"grp\"] = (\n",
    "        non_allowed_df.groupby(\"speaker\")[\"id\"]          # grupujemy po speaker\n",
    "        .transform(lambda x: pd.factorize(x)[0])  # 0 dla 1. id, 1 dla 2. id\n",
    "    )\n",
    "    non_allowed_df_train = non_allowed_df[non_allowed_df[\"grp\"] == 0].drop(columns=\"grp\").copy()\n",
    "    non_allowed_df_test = non_allowed_df[non_allowed_df[\"grp\"] == 1].drop(columns=\"grp\").copy()\n",
    "\n",
    "    #val-test split for allowed speakers\n",
    "    allowed_df_test_shuffled = (\n",
    "        allowed_df_test\n",
    "        .groupby(\"speaker\", group_keys=False)\n",
    "        .apply(lambda g: g.sample(frac=1, random_state=83))\n",
    "    )\n",
    "    allowed_df_test_shuffled[\"part\"] = (\n",
    "        allowed_df_test_shuffled.groupby(\"speaker\").cumcount() % 2\n",
    "    )\n",
    "    allowed_df_val = allowed_df_test_shuffled[allowed_df_test_shuffled[\"part\"] == 0].drop(columns=\"part\").copy()\n",
    "    allowed_df_test = allowed_df_test_shuffled[allowed_df_test_shuffled[\"part\"] == 1].drop(columns=\"part\").copy()\n",
    "\n",
    "    #val-test split for non-allowed speakers\n",
    "    non_allowed_df_test_shuffled = (\n",
    "        non_allowed_df_test\n",
    "        .groupby(\"speaker\", group_keys=False)\n",
    "        .apply(lambda g: g.sample(frac=1, random_state=83))\n",
    "    )\n",
    "    non_allowed_df_test_shuffled[\"part\"] = (\n",
    "        non_allowed_df_test_shuffled.groupby(\"speaker\").cumcount() % 2\n",
    "    )\n",
    "    non_allowed_df_val = non_allowed_df_test_shuffled[non_allowed_df_test_shuffled[\"part\"] == 0].drop(columns=\"part\").copy()\n",
    "    non_allowed_df_test = non_allowed_df_test_shuffled[non_allowed_df_test_shuffled[\"part\"] == 1].drop(columns=\"part\").copy()\n",
    "\n",
    "\n",
    "    train_df = pd.concat([allowed_df_train, non_allowed_df_train], ignore_index=True)\n",
    "    val_df = pd.concat([allowed_df_val, non_allowed_df_val], ignore_index=True)\n",
    "    test_df = pd.concat([allowed_df_test, non_allowed_df_test], ignore_index=True)\n",
    "    # sortowanie\n",
    "    train_df.to_csv(\"data/train_df.csv\", index=False)\n",
    "    val_df.to_csv(\"data/val_df.csv\", index=False)\n",
    "    test_df.to_csv(\"data/test_df.csv\", index=False)\n",
    "    print(f\"Number of speakers in the train set: {train_df['speaker'].nunique()}\")\n",
    "    print(f\"Number of speakers in the val set: {val_df['speaker'].nunique()}\")\n",
    "    print(f\"Number of speakers in the test set: {test_df['speaker'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff7473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speakers in the dataset: 66\n",
      "Number of segments in the dataset: 4224\n",
      "Number of speakers in the train set: 66\n",
      "Number of speakers in the val set: 66\n",
      "Number of speakers in the test set: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3351422/3339571101.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  allowed_df['label'] = 1\n",
      "/tmp/ipykernel_3351422/3339571101.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_allowed_df['label'] = 0\n",
      "/tmp/ipykernel_3351422/3339571101.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  allowed_df[\"grp\"] = (\n",
      "/tmp/ipykernel_3351422/3339571101.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_allowed_df[\"grp\"] = (\n",
      "/tmp/ipykernel_3351422/3339571101.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(frac=1, random_state=83))\n",
      "/tmp/ipykernel_3351422/3339571101.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(frac=1, random_state=83))\n"
     ]
    }
   ],
   "source": [
    "preprocess_data_non_leakage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b964e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_spectrogram_df(phase):\n",
    "    df = pd.read_csv(f'data/{phase}_df.csv')\n",
    "    df['filename'] = df['filename'].str.replace('speech', 'spectrograms_dataset', regex=False)\n",
    "    df['filename'] = df['filename'].str.replace('.wav', '.pt', regex=False)\n",
    "    df.to_csv(f'data/{phase}_spectogram_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d515c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_spectrogram_df('train')\n",
    "get_spectrogram_df('val')\n",
    "get_spectrogram_df('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'data/train_spectogram_df.csv')\n",
    "df_val = pd.read_csv(f'data/val_spectogram_df.csv')\n",
    "df_test = pd.read_csv(f'data/test_spectogram_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.speaker.nunique(), df_val.speaker.nunique(), df_test.speaker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a103e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_train.speaker.unique()) & set(df_test.speaker.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6504466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# znajdz różnicę symetryczną dwóch list\n",
    "set(df_train.speaker.unique()) ^ set(df_test.speaker.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a285bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# znajdz część wspólna dwóch list\n",
    "def find_common_elements(list1, list2):\n",
    "    \"\"\"\n",
    "    Find common elements between two lists.\n",
    "    \n",
    "    Args:\n",
    "        list1: First list of elements.\n",
    "        list2: Second list of elements.\n",
    "        \n",
    "    Returns:\n",
    "        List of common elements.\n",
    "    \"\"\"\n",
    "    return list(set(list1) & set(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_df.csv\")\n",
    "df_train.groupby(\"speaker\")[\"id\"].nunique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_df.csv\")\n",
    "df_val = pd.read_csv(\"data/val_df.csv\")\n",
    "df_test = pd.read_csv(\"data/test_df.csv\")\n",
    "df_test.speaker.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866897ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9572530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_df1.csv\")\n",
    "df_val = pd.read_csv(\"data/val_df1.csv\")\n",
    "df_test = pd.read_csv(\"data/test_df1.csv\")\n",
    "# df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "# df = df[['filename', 'speaker', 'gender']]\n",
    "df['filename'] = df['filename'].str.replace('VOiCES_devkit', 'spectrograms_dataset', regex=False)\n",
    "df['filename'] = df['filename'].str.replace('.wav', '.pt', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdcb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop_duplicates(subset=['filename']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858644ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"speaker\")[\"id\"].nunique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.speaker == 8152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b917a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in df.filename if os.path.exists(f) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['filename'].apply(lambda x: x[-6:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aece38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == '020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df.filename.apply(lambda x: x.split('-')[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mając dataframe df chce sprawdzić czy każdy speaker jest przypisany do dokładnie jednego id\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f88766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id_counts = df.groupby(\"speaker\")[\"id\"].nunique()\n",
    "speaker_id_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee03649",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id_counts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"grp\"] = (\n",
    "    df.groupby(\"speaker\")[\"id\"]          # grupujemy po speaker\n",
    "      .transform(lambda x: pd.factorize(x)[0])  # 0 dla 1. id, 1 dla 2. id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df[\"grp\"] == 0].drop(columns=\"grp\").copy()\n",
    "df2 = df[df[\"grp\"] == 1].drop(columns=\"grp\").copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e538c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby(\"speaker\")[\"id\"].nunique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb62280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clpi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
