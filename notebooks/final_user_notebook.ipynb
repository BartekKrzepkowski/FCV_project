{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301215e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sounddevice torchaudio ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409574c0",
   "metadata": {},
   "source": [
    "### 1. I am initializing the listening procedure. The program listens to me with the use of a laptop microphone and detects the class. Detected class and, optionally, confidence level, is displayed on the screen. The Jupyter-notebook program is used in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–€â–€â–€ 1. Importy i konfiguracja â–€â–€â–€\n",
    "import time, warnings\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import torch, torchaudio\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# model utils (u Ciebie w repo)\n",
    "from src.utils.utils_model import prepare_model\n",
    "\n",
    "SAMPLE_RATE  = 16_000\n",
    "DURATION_SEC = 1.0                   # 1-s fragment\n",
    "FRAME_LEN    = int(DURATION_SEC * SAMPLE_RATE)\n",
    "N_MELS, N_FFT, HOP = 80, 400, 160\n",
    "TARGET_FRAMES = 501\n",
    "CLASSES = [\"allowed\", \"non-allowed\"]\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# â–€â–€â–€ 2. Model â–€â–€â–€\n",
    "model_cfg = {\n",
    "    \"model_name\": \"flexible_cnn\",\n",
    "    \"model_params\": {\n",
    "        \"num_classes\": 2,\n",
    "        \"input_height\": 80,\n",
    "        \"input_time\": TARGET_FRAMES,\n",
    "        \"blocks_cfg\": [\n",
    "            dict(out_ch=16, activation=\"ReLU\", use_bn=False, dropout_p=0.0, skip=False, is_bn_pre_act=False),\n",
    "            dict(out_ch=32, activation=\"ReLU\", use_bn=False, dropout_p=0.0, skip=False, is_bn_pre_act=False),\n",
    "            dict(out_ch=32, activation=\"ReLU\", use_bn=False, dropout_p=0.0, skip=False, is_bn_pre_act=False),\n",
    "        ],\n",
    "    },\n",
    "    \"checkpoint_path\": \"best_model.pt\",\n",
    "    \"init\": None,\n",
    "    'freeze_backbone': False, \n",
    "}\n",
    "model = prepare_model(model_cfg).to(device).eval()\n",
    "\n",
    "# â–€â–€â–€ 3. Log-mel przetwarzanie â–€â–€â–€\n",
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP, n_mels=N_MELS, power=2.0\n",
    ")\n",
    "EPS = 1e-9\n",
    "def wav_to_logmel(wave: torch.Tensor) -> torch.Tensor:\n",
    "    if wave.dim() > 1:\n",
    "        wave = wave.mean(dim=0, keepdim=True)          # mono\n",
    "    rms = wave.pow(2).mean().sqrt()\n",
    "    wave = wave / (rms + EPS)\n",
    "    mel  = mel_spec(wave)\n",
    "    logm = torch.log(mel + EPS).squeeze(0)             # [80, T]\n",
    "\n",
    "    # dopasuj do 501 ramek\n",
    "    if logm.shape[-1] < TARGET_FRAMES:\n",
    "        pad = TARGET_FRAMES - logm.shape[-1]\n",
    "        logm = torch.nn.functional.pad(logm, (0, pad))\n",
    "    else:\n",
    "        logm = logm[..., :TARGET_FRAMES]\n",
    "    return logm                                         # [80, 501]\n",
    "\n",
    "# â–€â–€â–€ 4. PÄ™tla nasÅ‚uchu â–€â–€â–€\n",
    "def listen():\n",
    "    print(\"â–¶ï¸  Listeningâ€¦  (Ctrl-C w komÃ³rce, aby przerwaÄ‡)\")\n",
    "    while True:\n",
    "        audio = sd.rec(FRAME_LEN, samplerate=SAMPLE_RATE,\n",
    "                       channels=1, dtype=\"float32\")\n",
    "        sd.wait()\n",
    "        wave = torch.from_numpy(audio.T)                # [1, N]\n",
    "\n",
    "        spec = wav_to_logmel(wave).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(model(spec), dim=-1)[0].cpu()\n",
    "        conf, idx = torch.max(probs, dim=0)\n",
    "        clear_output(wait=True)\n",
    "        display(f\"ðŸŽ¤  **{CLASSES[idx]}**   |   confidence: {conf.item()*100:.1f}%\")\n",
    "        time.sleep(0.05)\n",
    "\n",
    "try:\n",
    "    listen()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"â¹ï¸  NasÅ‚uch zatrzymany.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from src.utils.utils_model import prepare_model\n",
    "\n",
    "# ====================== 1. PARAMETRY ============================\n",
    "SAMPLE_RATE     = 16_000\n",
    "DURATION        = 1.0          # sekundy\n",
    "FRAME_LEN       = int(DURATION * SAMPLE_RATE)\n",
    "DEVICE          = None         # None = domyÅ›lny mikrofon\n",
    "USE_MIC         = False         # False â†’ fallback z szumem\n",
    "N_MELS          = 80\n",
    "N_FFT           = 400\n",
    "HOP_LENGTH      = 160\n",
    "TARGET_FRAMES   = 501\n",
    "EPS             = 1e-9\n",
    "CLASSES         = [\"allowed\", \"non-allowed\"]\n",
    "\n",
    "# ====================== 2. MODEL ================================\n",
    "use_bn      = True\n",
    "dropout_p   = 0.0\n",
    "activation  = \"ReLU\"\n",
    "is_bn_pre_act = True\n",
    "skip        = False\n",
    "optim_name   = \"adamw\"\n",
    "lr          = 1e-3\n",
    "weight_decay = 1e-1\n",
    "\n",
    "model_params = {\n",
    "    'model_name': 'flexible_cnn',\n",
    "    'model_params': {\n",
    "        'num_classes': 2,\n",
    "        'input_height': 80,\n",
    "        'input_time': 501,\n",
    "        'blocks_cfg': [\n",
    "            dict(out_ch=16, use_bn=use_bn, dropout_p=dropout_p, skip=False, activation=activation, is_bn_pre_act=is_bn_pre_act),\n",
    "            dict(out_ch=32, use_bn=use_bn, dropout_p=dropout_p, skip=False, activation=activation, is_bn_pre_act=is_bn_pre_act),\n",
    "            dict(out_ch=32, use_bn=use_bn, dropout_p=dropout_p, skip=skip, activation=activation, is_bn_pre_act=is_bn_pre_act),\n",
    "        ],       \n",
    "    },\n",
    "    'checkpoint_path': f\"models/best_model_{optim_name}_{lr}_{weight_decay}_{activation}_{dropout_p}_{use_bn}_{is_bn_pre_act}_{skip}.pt\",\n",
    "    'init': None\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = prepare_model(model_params).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ====================== 3. TRANSFORM ============================\n",
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    power=2.0,\n",
    ")\n",
    "\n",
    "def waveform_to_logmel(waveform: torch.Tensor) -> torch.Tensor:\n",
    "    if waveform.dim() > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # PAD przed MelSpectrogram (min. n_fft)\n",
    "    if waveform.shape[-1] < N_FFT:\n",
    "        pad = N_FFT - waveform.shape[-1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad))\n",
    "\n",
    "    # RMS normalizacja\n",
    "    rms = waveform.pow(2).mean().sqrt()\n",
    "    waveform = waveform / (rms + EPS)\n",
    "\n",
    "    mel = mel_spec(waveform)\n",
    "    logmel = torch.log(mel + EPS).squeeze(0)\n",
    "\n",
    "    # PAD lub przyciÄ™cie do TARGET_FRAMES\n",
    "    frames = logmel.shape[-1]\n",
    "    if frames < TARGET_FRAMES:\n",
    "        logmel = torch.nn.functional.pad(logmel, (0, TARGET_FRAMES - frames))\n",
    "    else:\n",
    "        logmel = logmel[..., :TARGET_FRAMES]\n",
    "\n",
    "    return logmel\n",
    "\n",
    "# ====================== 4. NAGRYWANIE ============================\n",
    "def record_audio():\n",
    "    \"\"\"Zwraca waveform [1, N] dÅ‚ugoÅ›ci â‰¥ FRAME_LEN.\"\"\"\n",
    "    if USE_MIC:\n",
    "        audio = sd.rec(FRAME_LEN, samplerate=SAMPLE_RATE, channels=1,\n",
    "                       dtype=\"float32\", device=DEVICE)\n",
    "        sd.wait()\n",
    "        return torch.from_numpy(audio.T)\n",
    "    else:\n",
    "        return 0.01 * torch.randn(1, FRAME_LEN)  # fallback\n",
    "\n",
    "# ====================== 5. NASÅUCH ==============================\n",
    "def listen_loop():\n",
    "    print(\"â–¶ï¸  Listeningâ€¦  (Ctrl-C to stop)\")\n",
    "    while True:\n",
    "        waveform = record_audio()\n",
    "        spec = waveform_to_logmel(waveform)                # [80, 501]\n",
    "        spec = spec.unsqueeze(0).unsqueeze(0).to(device)   # [1, 1, 80, 501]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(spec)\n",
    "            probs = torch.softmax(logits, dim=-1)[0]\n",
    "\n",
    "        conf, idx = torch.max(probs, dim=0)\n",
    "        label = CLASSES[idx]\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(f\"ðŸŽ¤  **{label}**  |  confidence: {conf.item()*100:.1f}%\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "try:\n",
    "    listen_loop()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"â¹ï¸  Zatrzymano nasÅ‚uch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28c5fd",
   "metadata": {},
   "source": [
    "### 2. I am running appropriate fragment of the code to add one more person to class 1. The system may take some time to process new data (update the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193d9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089696f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_speaker(all_speakers, old_speakers):\n",
    "    \"\"\"\n",
    "    Wybiera nowego speakera spoÅ›rÃ³d dostÄ™pnych, z wykluczeniem starych.\n",
    "    \"\"\"\n",
    "    available_speakers = [spk for spk in all_speakers if spk not in old_speakers]\n",
    "    if not available_speakers:\n",
    "        raise ValueError(\"Brak dostÄ™pnych nowych speakerÃ³w.\")\n",
    "    return random.choice(available_speakers)\n",
    "\n",
    "def get_spectrogram_df(phase):\n",
    "    df = pd.read_csv(f'data/{phase}_df.csv')\n",
    "    df['filename'] = df['filename'].str.replace('speech', 'spectrograms_dataset', regex=False)\n",
    "    df['filename'] = df['filename'].str.replace('.wav', '.pt', regex=False)\n",
    "    df.to_csv(f'data/{phase}_spectogram_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcfb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def create_new_speaker_spectogram_data():\n",
    "    \"\"\"\n",
    "    Tworzy dane dla nowego speakera.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(\"train_index.csv\")\n",
    "    df_test = pd.read_csv(\"test_index.csv\")\n",
    "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    df = df[['filename', 'speaker', 'gender']]\n",
    "    df['id'] = df.filename.apply(lambda x: x.split('-')[-5])\n",
    "    df_filtered = df[df.groupby(\"speaker\")[\"id\"].transform(\"nunique\") == 2]\n",
    "\n",
    "    old_speakers = pd.read_csv(f'data/train_spectogram_df.csv').speaker.unique().tolist()\n",
    "    new_speaker = choose_new_speaker(df_filtered.speaker.unique(), old_speakers)\n",
    "\n",
    "    new_speaker_data = df_filtered[df_filtered.speaker == new_speaker].copy()\n",
    "    new_speaker_data['label'] = 1\n",
    "    new_speaker_data.to_csv('data/new_speaker_df.csv', index=False)\n",
    "    get_spectrogram_df('new_speaker')\n",
    "\n",
    "    # train-test split po dwÃ³ch rodzajach nagrania dla kaÅ¼dego speakera\n",
    "    # allowed speakers\n",
    "    new_speaker_data[\"grp\"] = (\n",
    "        new_speaker_data.groupby(\"speaker\")[\"id\"]          # grupujemy po speaker\n",
    "        .transform(lambda x: pd.factorize(x)[0])  # 0 dla 1. id, 1 dla 2. id\n",
    "    )\n",
    "    new_speaker_data_train = new_speaker_data[new_speaker_data[\"grp\"] == 0].drop(columns=\"grp\").copy()\n",
    "    new_speaker_data_test = new_speaker_data[new_speaker_data[\"grp\"] == 1].drop(columns=\"grp\").copy()\n",
    "\n",
    "    #val-test split for allowed speakers\n",
    "    new_speaker_data_test_shuffled = (\n",
    "        new_speaker_data_test\n",
    "        .groupby(\"speaker\", group_keys=False)\n",
    "        .apply(lambda g: g.sample(frac=1, random_state=83))\n",
    "    )\n",
    "    new_speaker_data_test_shuffled[\"part\"] = (\n",
    "        new_speaker_data_test_shuffled.groupby(\"speaker\").cumcount() % 2\n",
    "    )\n",
    "    new_speaker_data_val = new_speaker_data_test_shuffled[new_speaker_data_test_shuffled[\"part\"] == 0].drop(columns=\"part\").copy()\n",
    "    new_speaker_data_test = new_speaker_data_test_shuffled[new_speaker_data_test_shuffled[\"part\"] == 1].drop(columns=\"part\").copy()\n",
    "\n",
    "    new_speaker_data_train.to_csv('data/new_speaker_train_df.csv', index=False)\n",
    "    new_speaker_data_val.to_csv('data/new_speaker_val_df.csv', index=False)\n",
    "    new_speaker_data_test.to_csv('data/new_speaker_test_df.csv', index=False)\n",
    "\n",
    "    get_spectrogram_df('new_speaker_train')\n",
    "    get_spectrogram_df('new_speaker_val')\n",
    "    get_spectrogram_df('new_speaker_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c9ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3321541/1117272555.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(frac=1, random_state=83))\n"
     ]
    }
   ],
   "source": [
    "create_new_speaker_spectogram_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd36909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_datasets():\n",
    "    old_df_train = pd.read_csv(\"data/train_df.csv\")\n",
    "    old_df_val = pd.read_csv(\"data/val_df.csv\")\n",
    "    old_df_test = pd.read_csv(\"data/test_df.csv\")\n",
    "    new_speaker_data_train = pd.read_csv(\"data/new_speaker_train_df.csv\")\n",
    "    new_speaker_data_val = pd.read_csv(\"data/new_speaker_val_df.csv\")\n",
    "    new_speaker_data_test = pd.read_csv(\"data/new_speaker_test_df.csv\")\n",
    "\n",
    "\n",
    "    replay_rows_train = []\n",
    "    for _, grp in old_df_train[old_df_train['label'] == 1].groupby(\"speaker\"):\n",
    "        replay_rows_train.append(grp.sample(frac=0.10, random_state=42))\n",
    "    for _, grp in old_df_train[old_df_train['label'] == 0].groupby(\"speaker\"):\n",
    "        replay_rows_train.append(grp.sample(frac=0.10, random_state=42))\n",
    "    replay_train_df = pd.concat(replay_rows_train)\n",
    "\n",
    "    # replay_rows_val = []\n",
    "    # for _, grp in old_df_val[old_df_val['label'] == 1].groupby(\"speaker\"):\n",
    "    #     replay_rows_val.append(grp.sample(frac=0.10, random_state=42))\n",
    "    # for _, grp in old_df_val[old_df_val['label'] == 0].groupby(\"speaker\"):\n",
    "    #     replay_rows_val.append(grp.sample(frac=0.10, random_state=42))\n",
    "    # replay_val_df = pd.concat(replay_rows_val)\n",
    "    replay_val_df = old_df_val\n",
    "\n",
    "    # replay_rows_test = []\n",
    "    # for _, grp in old_df_test[old_df_test['label'] == 1].groupby(\"speaker\"):\n",
    "    #     replay_rows_test.append(grp.sample(frac=0.10, random_state=42))\n",
    "    # for _, grp in old_df_test[old_df_test['label'] == 0].groupby(\"speaker\"):\n",
    "    #     replay_rows_test.append(grp.sample(frac=0.10, random_state=42))\n",
    "    # replay_test_df = pd.concat(replay_rows_test)\n",
    "    replay_test_df = old_df_test\n",
    "\n",
    "    train_df = pd.concat([new_speaker_data_train, replay_train_df], ignore_index=True)\n",
    "    val_df = pd.concat([new_speaker_data_val, replay_val_df], ignore_index=True)\n",
    "    test_df = pd.concat([new_speaker_data_test, replay_test_df], ignore_index=True)\n",
    "\n",
    "    train_df.to_csv('data/1new_speaker_train_df.csv', index=False)\n",
    "    val_df.to_csv('data/1new_speaker_val_df.csv', index=False)\n",
    "    test_df.to_csv('data/1new_speaker_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2783a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba5973d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_spectrogram_df('1new_speaker_train')\n",
    "get_spectrogram_df('1new_speaker_val')\n",
    "get_spectrogram_df('1new_speaker_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7b08a",
   "metadata": {},
   "source": [
    "## Then run\n",
    "### sbatch run_main_new_speaker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca93b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15a4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.utils_model import prepare_model\n",
    "from src.utils.utils_data import prepare_loaders\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838a9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = {\n",
    "    \"model_name\": \"flexible_cnn\",\n",
    "    \"model_params\": {\n",
    "        \"num_classes\": 2,\n",
    "        \"input_height\": 80,\n",
    "        \"input_time\": 501,\n",
    "        \"blocks_cfg\": [\n",
    "            dict(out_ch=16, activation=\"ReLU\", use_bn=True, dropout_p=0.0, skip=False, is_bn_pre_act=True),\n",
    "            dict(out_ch=32, activation=\"ReLU\", use_bn=True, dropout_p=0.0, skip=False, is_bn_pre_act=True),\n",
    "            dict(out_ch=32, activation=\"ReLU\", use_bn=True, dropout_p=0.0, skip=False, is_bn_pre_act=True),\n",
    "        ],\n",
    "    },\n",
    "    \"checkpoint_path\": \"/net/people/plgrid/plgkrzepk/newGithub/FCV_project/models/best_model__one_speaker_adamw_0.0001_0.1_ReLU_0.0_True_True_False.pt\",\n",
    "    \"init\": None,\n",
    "    'freeze_backbone': False, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2946f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/people/plgrid/plgkrzepk/newGithub/FCV_project/src/utils/utils_general.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model(model_cfg).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45345bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:56<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model F1-score on test set: 76.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_params = {\n",
    "    'dataset_name' : 'voices_spectograms',\n",
    "    'dataset_params': {\n",
    "        'custom_root': '/net/pr2/projects/plgrid/plggdnnp/datasets/VOiCES_devkit',\n",
    "        'df_train_path': 'data/train_spectogram_df.csv',\n",
    "        'df_val_path': 'data/val_spectogram_df.csv',\n",
    "        'df_test_path': 'data/test_spectogram_df.csv',\n",
    "        'use_transform': True, # if True, then use transforms for the base dataset (per side [CIFAR10 at this point])\n",
    "    },\n",
    "    'loader_params': {'batch_size': 128, 'pin_memory': True, 'num_workers': 12}\n",
    "}\n",
    "loaders = prepare_loaders(data_params)\n",
    "test_loader = loaders['test']\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            x_true, y_true = data\n",
    "            inputs, labels = x_true.to(device), y_true.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')  # lub 'macro' dla wieloklasowej\n",
    "    return f1\n",
    "\n",
    "f1 = evaluate_model(model, test_loader)\n",
    "print(f\"Model F1-score on test set: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e74157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model F1-score on test set: 76.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils.utils_data import prepare_loaders\n",
    "data_params = {\n",
    "    'dataset_name' : 'voices_spectograms',\n",
    "    'dataset_params': {\n",
    "        'custom_root': '/net/pr2/projects/plgrid/plggdnnp/datasets/VOiCES_devkit',\n",
    "        'df_train_path': 'data/new_speaker_train_spectogram_df.csv',\n",
    "        'df_val_path': 'data/new_speaker_val_spectogram_df.csv',\n",
    "        'df_test_path': 'data/new_speaker_test_spectogram_df.csv',\n",
    "        'use_transform': True, # if True, then use transforms for the base dataset (per side [CIFAR10 at this point])\n",
    "    },\n",
    "    'loader_params': {'batch_size': 128, 'pin_memory': True, 'num_workers': 12}\n",
    "}\n",
    "loaders = prepare_loaders(data_params)\n",
    "test_loader = loaders['test']\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            x_true, y_true = data\n",
    "            inputs, labels = x_true.to(device), y_true.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')  # lub 'macro' dla wieloklasowej\n",
    "    return f1\n",
    "\n",
    "f1 = evaluate_model(model, test_loader)\n",
    "print(f\"Model F1-score on test set: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afd7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clpi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
